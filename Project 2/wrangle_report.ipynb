{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Reporting: Dog Ratings Data Wrangling and Analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the four steps taken in this Wrangling\n",
    "1. Introduction\n",
    "2. Data gathering\n",
    "3. Data Assessment\n",
    "4. Data cleasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a project to gather data about Dog ratings from 3 different sources namely: WeRateDogs Twitter archive data, the tweet image prediction using the Requests library and from the Twitter API. The data is to be read into 3 different dataframes using differents methods that are stated in the project rubik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA GATHERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step i took in data gathering is to import all the libraries that i needed for the whole wrangling and analysis process. \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import wptools \n",
    "\n",
    "import os \n",
    "\n",
    "import requests \n",
    "\n",
    "from PIL import Image \n",
    "\n",
    "from io import BytesIO \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of the data i gathered is from the WeRateDogs Twitter archive. This data is provided in the Udacity class in a csv file called twitter-archive-enhanced.csv. I read the data into a dataframe\n",
    "df = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded the second set using request library then read it into a dataframe using:\n",
    "df_image = pd.read_csv('tweet_image_prediction/image-predictions.tsv', sep = '\\t') taking note of the type of file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tweepy to query the Twitter API and save it into a json file,\n",
    "I used json to read the json file into a dictionary then into a dataframe:\n",
    "\n",
    "with open('tweet-json.txt', 'r') as f: \n",
    "\n",
    "  dictlist = [json.loads(x) for x in f] \n",
    "  \n",
    "df_json=pd.DataFrame(dictlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ASSESSMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assessment of the data was done using visual assessment and programatic assessment. Some of the assessment codes are:\n",
    "df.head\n",
    "\n",
    "df_image.head\n",
    "\n",
    "df_json.head\n",
    "\n",
    "df.info\n",
    "\n",
    "df_image.info\n",
    "\n",
    "df_json.info\n",
    "\n",
    "df_image.img_num.value_counts()\n",
    "\n",
    "df_image[df_image.tweet_id.duplicated()]\n",
    "\n",
    "all_columns = pd.Series(list(df) + list(df_image) + list(df_json))\n",
    "all_columns[all_columns.duplicated()] among others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the different codes i was able to list some of the data quality issues and tidyness issues that i was going to clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issues\n",
    "1. df'timestamp' and 'source' is in wrong datatype\n",
    "\n",
    "2. df'source' still has the html residues\n",
    "\n",
    "3. df'timestamp, text, name, source', df_image'p1, p2, p3 and their conf' do not have a clear descriptive heading\n",
    "\n",
    "4. Reduce the decimal points in the p1_conf, p2_conf and p3_conf\n",
    "\n",
    "5. df_json[id] does not have the correct naming. it has the tweet id so the heading needs to be changed\n",
    "\n",
    "6. df_json columns that are not needed should be dropped\n",
    "\n",
    "7. df dataframe has retweets. We only need the original tweets\n",
    "\n",
    "8. df dataframe columns that are not needed should be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness issues\n",
    "1. The information in the df_json dataframe and df_image dataframe is needed in the df dataframe\n",
    "\n",
    "2. The stages of dog is spread out into four columns instead of one column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data cleansing format took the process of \n",
    "1. Making a copy of all the original data\n",
    "2. Highlight each issue, Define the mode of cleaning, Write the code to clean and test after cleaning is done to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STORING\n",
    "After the cleaning process is done the cleaned data, i stored the data in the twitter_archive_master.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
